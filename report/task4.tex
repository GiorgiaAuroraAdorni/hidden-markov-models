\Problem{
	You are given three data points $(x, y) : (-1, 0), (0, 1), (1, 0)$. We are
	using squared error loss function 
	$\ell(y, \hat y) = (y - \hat y)^2$.
	
	\begin{itemize}
		\item[(a)] What is leave-one-out cross-validation error of constant 
		model $f(x) = c$?
		\item[(b)] What is leave-one-out cross-validation error of linear model 
		$f(x) = ax + b$?
	\end{itemize}
}

\Solution{
	\begin{itemize}
		\item[(a)] In each step, the model is trained on $n-1$ data-points and 
		validated on the other. First of all, the train set loss is computed:
		
		\begin{align*}
			l_1(c_1) & =  (0-{c_1})^2 + (1-{c_1})^2 = 2 {c_1}^2 - 2 {c_1} + 1\\
			l_2(c_2) & = (0-{c_2})^2 + (0-{c_2})^2 = 2{c_2}^2  \\
			l_3(c_3) & = (1-{c_3})^2 + (0-{c_3})^2 =  2{c_3}^2 -2{c_3} +1  
		\end{align*}
		
		Since we want to find the value of $c$ that minimise the squared error, 
		it is necessary to compute the derivative of the loss $l$ with respect 
		to $c$:
		 
		\begin{align*}
		\frac{\partial l_1}{\partial c_1} & = 4c_1 -2 = 0  
		& c_1 = \frac{1}{2} \\
		\frac{\partial l_2}{\partial c_2} &= 4c_2 = 0 
		& c_2 = 0 \\
		\frac{\partial l_3}{\partial c_3} & = 4c_3 -2 = 0  
		& c_3 = \frac{1}{2}
		\end{align*}
		
		Now, it is possible to validate the model on the other data-point:
		\begin{align*}
		e_1(c_1) &=  \bigg(0-c_1\bigg)^2 =  \bigg(0-\frac{1}{2}\bigg)^2 = 
		\frac{1}{4} \\
		e_2(c_2) &=  \bigg(0-c_2\bigg)^2 =  \bigg(1 - 0 \bigg)^2 = 1 \\
		e_3(c_3) &=  \bigg(0-c_3\bigg)^2 =  \bigg(0 - \frac{1}{2} \bigg)^2 = 
		\frac{1}{4}
		\end{align*}
		
		The final error is computed as the mean of the validation errors: 
		\begin{equation*}
			E=\frac{e_1+e_2+e_3}{3}=\bigg(\frac{1}{4}+1+\frac{1}{4}\bigg) 
			\frac{1}{3} = \frac{3}{2}\cdot \frac{1}{3}=\frac{1}{2}=0.5
		\end{equation*}
		
		\item[(b)] As in the previous exercise, in each step, the model is 
		trained on $n-1$ data-points and validated on the other. First of all, 
		the train set loss is computed:
		
		\begin{align*}
		l_1(a_1x_1+b_1) & = \big(0-({-a_1+b_1})\big)^2 + 
		\big(1-b_1\big)^2  =  a_1^2 - 2 a_1 b_1 + 2 b_1^2 - 2 b_1 + 1  \\
		l_2(a_2x_2+b_2) & = \big(0-({-a_2+b_2})\big)^2 + 
		\big(0-({a_2+b_2})\big)^2 = 2 a_2^2 + 2 b_2^2 \\
		l_3(a_3x_3+b_3) & = \big(1-b_1\big)^2 + 
		\big(0-({a_2+b_2})\big)^2 = a_3^2 + 2 a_3 b_3 + 2 b_3^2 - 2 b_3 + 1
		\end{align*}
		
		Since we want to find the values of $a$ and $b$ that minimise the 
		squared error, it is necessary to compute the derivative of the loss 
		$l$ with respect to $a$ and $b$:
		
		\begin{align*}
		&\frac{\partial l_1}{\partial a_1} = 2 a_1 - 2 b_1 = 0  
		& a_1 = b_1 \\
		&\frac{\partial l_1}{\partial b_1} = -2 a_1 + 4 b_1 - 2 = 0  
		& b_1 = \frac{1}{2} (a_1 + 1)
		\end{align*}
		\begin{align*}
		a_1 &= \frac{1}{2} (a_1 + 1)  &\rightarrow &\quad a_1 = 1\\
		b_1 &= \frac{1}{2} (1 + 1)  &\rightarrow& \quad b_1 = 1
		\end{align*}
		
		\begin{align*}
		\frac{\partial l_2}{\partial a_2} & = 4 a_2 = 0  
		&\rightarrow &\quad a_2 = 0  \\
		\frac{\partial l_2}{\partial b_2} & = 4 b_2 = 0  
		&\rightarrow &\quad b_2 = 0
		\end{align*}
		\begin{align*}
		\frac{\partial l_3}{\partial a_3} & = 2a_3+2b_3= 0  
		& a_3 = -b_3 \\
		\frac{\partial l_3}{\partial b_3} & = 2 a_3 + 4 b_3 - 2 = 0  
		& b_3 = \frac{1}{2} - \frac{a_3}{2}
		\end{align*}
		
		\begin{align*}
		a_3 &= - \bigg(\frac{1}{2} - \frac{a_3}{2}\bigg) &\rightarrow &\quad 
		a_3 = -1\\
		b_3 &= \frac{1}{2} -\bigg(-\frac{1}{2}\bigg) &\rightarrow& \quad b_3 = 1
		\end{align*}
		
		Now, it is possible to validate the model on the other data-point:
		\begin{align*}
		a_1x_1+b_1 & =  1 \cdot 1 + 1 = 2   \\
		a_2x_2+b_2 & =  0 \cdot 0 + 0 = 0   \\
		a_3x_3+b_3 & = -1 \cdot -1 + 1 = 2
		\end{align*}
		\begin{align*}
		e_1(a_1x_1+b_1) & = \big(0-(a_1x_1+b_1)\big)^2 = (0-2)^2 = 4\\
		e_2(a_2x_2+b_2) & = \big(1-(a_2x_2+b_2)\big)^2 = (1-0)^2 = 1\\
		e_3(a_3x_3+b_3) & = \big(0-(a_3x_3+b_3)\big)^2 = (0-2)^2 = 4
		\end{align*}
		
		The final error is computed as the mean of the validation errors: 
		\begin{equation*}
		E=\frac{e_1+e_2+e_3}{3}=\frac{4+1+4}{3}=\frac{9}{3}=3
		\end{equation*}
		
	\end{itemize}
}